{"cells":[{"cell_type":"code","source":["'''\nPySpark Join is used to combine two DataFrames and by chaining these you can join multiple DataFrames; it supports all basic join type operations available in traditional SQL like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN. PySpark Joins are wider transformations that involve data shuffling across the network.\n\nPySpark SQL Joins comes with more optimization by default (thanks to DataFrames) however still there would be some performance issues to consider while using.\n\nIn this PySpark SQL Join tutorial, you will learn different Join syntaxes and using different Join types on two or more DataFrames and Datasets using examples.\n\nPySpark Join Syntax\nPySpark Join Types\nInner Join DataFrame\nFull Outer Join DataFrame\nLeft Outer Join DataFrame\nRight Outer Join DataFrame\nLeft Anti Join DataFrame\nLeft Semi Join DataFrame\nSelf Join DataFrame\nUsing SQL Expression\n\n1. PySpark Join Syntax\nPySpark SQL join has a below syntax and it can be accessed directly from DataFrame.\njoin() operation takes parameters as below and returns DataFrame.\n\njoin(self, other, on=None, how=None)\n\nparam other: Right side of the join\nparam on: a string for the join column name\nparam how: default inner. Must be one of inner, cross, outer,full, full_outer, left, left_outer, right, right_outer,left_semi, and left_anti.\nYou can also write Join expression by adding where() and filter() methods on DataFrame and can have Join on multiple columns.\n\n2. PySpark Join Types\nBelow are the different Join Types PySpark supports.\n\nJoin String\t                         Equivalent SQL Join\ninner\t                             INNER JOIN\nouter, full, fullouter, full_outer\t FULL OUTER JOIN\nleft, leftouter, left_outer\t         LEFT JOIN\nright, rightouter, right_outer\t     RIGHT JOIN\ncross\t\nanti, leftanti, left_anti\t\nsemi, leftsemi, left_semi\t\n\nBefore we jump into PySpark SQL Join examples, first, let’s create an \"emp\" and \"dept\" DataFrames. here, column \"emp_id\" is unique on emp and \"dept_id\" is unique on the dept dataset’s and emp_dept_id from emp has a reference to dept_id on dept dataset.\n'''\n\nemp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n  ]\nempColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n       \"emp_dept_id\",\"gender\",\"salary\"]\n\nempDF = spark.createDataFrame(data=emp, schema = empColumns)\nempDF.printSchema()\nempDF.show(truncate=False)\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n\n'''This prints “emp” and “dept” DataFrame to the console. Refer complete example below on how to create spark object.'''\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe715736-6b77-45e0-a5ed-806220d41928"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\nOut[3]: 'This prints “emp” and “dept” DataFrame to the console. Refer complete example below on how to create spark object.'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\nOut[3]: 'This prints “emp” and “dept” DataFrame to the console. Refer complete example below on how to create spark object.'"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n3. PySpark Inner Join DataFrame\nInner join is the default join in PySpark and it’s mostly used. This joins two datasets on key columns, where keys don’t match the rows get dropped from both datasets (emp & dept).\n\nWhen we apply Inner join on our datasets, It drops “emp_dept_id” 50 from “emp” and “dept_id” 30 from “dept” datasets. Below is the result of the above Join expression.\n'''\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n     .show(truncate=False)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0ca9d85-8333-4d92-90e5-db87f05afa5d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n4. PySpark Full Outer Join\nOuter a.k.a full, fullouter join returns all rows from both datasets, where join expression doesn’t match it returns null on respective record columns.\n\nFrom our “emp” dataset’s “emp_dept_id” with value 50 doesn’t have a record on “dept” hence dept columns have null and “dept_id” 30 doesn’t have a record in “emp” hence you see null’s on emp columns. Below is the result of the below Join expression.\n'''\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n    .show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8be8489-573b-4a02-9397-048b54bd6894"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":[" '''\n  5. PySpark Left Outer Join\nLeft a.k.a Leftouter join returns all rows from the left dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found.\n\nFrom our dataset, “emp_dept_id” 50 doesn’t have a record on “dept” dataset hence, this record contains null on “dept” columns (dept_name & dept_id). and “dept_id” 30 from “dept” dataset dropped from the results. Below is the result of the above Join expression.\n  '''\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\") \\\n    .show(truncate=False)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1266cee8-005e-4677-a083-2d5afd53dd3b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n6. Right Outer Join\nRight a.k.a Rightouter join is opposite of left join, here it returns all rows from the right dataset regardless of math found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found.\n\nFrom our example, the right dataset “dept_id” 30 doesn’t have it on the left dataset “emp” hence, this record contains null on “emp” columns. and “emp_dept_id” 50 dropped as a match not found on left. Below is the result of the above Join expression.\n'''\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n   .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n   .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8676682-c2c1-4736-8559-1eae98303edb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n7. Left Semi Join\nleftsemi join is similar to inner join difference being leftsemi join returns all columns from the left dataset and ignores all columns from the right dataset. In other words, this join returns columns from the only left dataset for the records match in the right dataset on join expression, records not matched on join expression are ignored from both left and right datasets.\n\nThe same result can be achieved using select on the result of the inner join however, using this join would be efficient.\nBelow is the result of the above join expression.\n'''\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n   .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbf8d5ad-ab76-4f72-bf7a-4a349b8380ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n8. Left Anti Join\nleftanti join does the exact opposite of the leftsemi, leftanti join returns only columns from the left dataset for non-matched records.\n\nYields below output\n'''\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n   .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16a83bf8-0eca-4fb2-8eb5-9998492b4fc1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1715d667-1cde-40db-a662-13946dfffae0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["'''\nJoins are not complete without a self join, Though there is no self-join type available, we can use any of the above-explained join types to join DataFrame to itself. below example use inner self join.\n\nHere, we are joining emp dataset with itself to find out superior emp_id and name for all employees.\n'''\nempDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n   .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"868410b9-68f5-4a68-a1e1-2f08edb01987"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------------+\n|emp_id|name    |superior_emp_id|superior_emp_name|\n+------+--------+---------------+-----------------+\n|2     |Rose    |1              |Smith            |\n|3     |Williams|1              |Smith            |\n|4     |Jones   |2              |Rose             |\n|5     |Brown   |2              |Rose             |\n|6     |Brown   |2              |Rose             |\n+------+--------+---------------+-----------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------------+\n|emp_id|name    |superior_emp_id|superior_emp_name|\n+------+--------+---------------+-----------------+\n|2     |Rose    |1              |Smith            |\n|3     |Williams|1              |Smith            |\n|4     |Jones   |2              |Rose             |\n|5     |Brown   |2              |Rose             |\n|6     |Brown   |2              |Rose             |\n+------+--------+---------------+-----------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n4. Using SQL Expression\nSince PySpark SQL support native SQL syntax, we can also write join operations after creating temporary tables on DataFrames and use these tables on spark.sql().\n\n\n'''\nempDF.createOrReplaceTempView(\"EMP\")\ndeptDF.createOrReplaceTempView(\"DEPT\")\n\njoinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False)\n\njoinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99848d37-3c9c-4ea7-b210-f3c0ed89e8ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n6. PySpark SQL Join Complete Example\n'''\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nemp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n  ]\nempColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n       \"emp_dept_id\",\"gender\",\"salary\"]\n\nempDF = spark.createDataFrame(data=emp, schema = empColumns)\nempDF.printSchema()\nempDF.show(truncate=False)\n\n\ndept = [(\"Finance\",10), \\\n    (\"Marketing\",20), \\\n    (\"Sales\",30), \\\n    (\"IT\",40) \\\n  ]\ndeptColumns = [\"dept_name\",\"dept_id\"]\ndeptDF = spark.createDataFrame(data=dept, schema = deptColumns)\ndeptDF.printSchema()\ndeptDF.show(truncate=False)\n  \nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n     .show(truncate=False)\n\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n    .show(truncate=False)\n    \nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n    .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\") \\\n   .show(truncate=False)\n\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n   .show(truncate=False)\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n   .show(truncate=False)\n\nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n   .show(truncate=False)\n   \nempDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n   .show(truncate=False)\n   \nempDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n   .show(truncate=False)\n\nempDF.createOrReplaceTempView(\"EMP\")\ndeptDF.createOrReplaceTempView(\"DEPT\")\n   \njoinDF = spark.sql(\"select * from EMP e, DEPT d where e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False)\n\njoinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n  .show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca38d750-0d2f-4e92-a3f0-b53ac45f0ab5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n+------+--------+---------------+-----------------+\n|emp_id|name    |superior_emp_id|superior_emp_name|\n+------+--------+---------------+-----------------+\n|2     |Rose    |1              |Smith            |\n|3     |Williams|1              |Smith            |\n|4     |Jones   |2              |Rose             |\n|5     |Brown   |2              |Rose             |\n|6     |Brown   |2              |Rose             |\n+------+--------+---------------+-----------------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n|6     |Brown   |2              |2010       |50         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\nroot\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|Finance  |10     |\n|Marketing|20     |\n|Sales    |30     |\n|IT       |40     |\n+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n|6     |Brown   |2              |2010       |50         |      |-1    |null     |null   |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|null  |null    |null           |null       |null       |null  |null  |Sales    |30     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |\n|3     |Williams|1              |2010       |10         |M     |1000  |\n|4     |Jones   |2              |2005       |10         |F     |2000  |\n|2     |Rose    |1              |2010       |20         |M     |4000  |\n|5     |Brown   |2              |2010       |40         |      |-1    |\n+------+--------+---------------+-----------+-----------+------+------+\n\n+------+-----+---------------+-----------+-----------+------+------+\n|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|6     |Brown|2              |2010       |50         |      |-1    |\n+------+-----+---------------+-----------+-----------+------+------+\n\n+------+--------+---------------+-----------------+\n|emp_id|name    |superior_emp_id|superior_emp_name|\n+------+--------+---------------+-----------------+\n|2     |Rose    |1              |Smith            |\n|3     |Williams|1              |Smith            |\n|4     |Jones   |2              |Rose             |\n|5     |Brown   |2              |Rose             |\n|6     |Brown   |2              |Rose             |\n+------+--------+---------------+-----------------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\nPySpark distinct() function is used to drop/remove the duplicate rows (all columns) from DataFrame and dropDuplicates() is used to drop rows based on selected (one or multiple) columns. In this article, you will learn how to use distinct() and dropDuplicates() functions with PySpark example.\n\nBefore we start, first let’s create a DataFrame with some duplicate rows and values on a few columns. We use this DataFrame to demonstrate how to get distinct multiple columns.\n\nOn the above table, record with employer name James has duplicate rows, As you notice we have 2 rows that have duplicate values on all columns and we have 4 rows that have duplicate values on department and salary columns.\n\n'''\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndata = [(\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600), \\\n    (\"Robert\", \"Sales\", 4100), \\\n    (\"Maria\", \"Finance\", 3000), \\\n    (\"James\", \"Sales\", 3000), \\\n    (\"Scott\", \"Finance\", 3300), \\\n    (\"Jen\", \"Finance\", 3900), \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000), \\\n    (\"Saif\", \"Sales\", 4100) \\\n  ]\ncolumns= [\"employee_name\", \"department\", \"salary\"]\ndf = spark.createDataFrame(data = data, schema = columns)\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6449da25-eb20-47ea-8e64-3c60c33703bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n1. Get Distinct Rows (By Comparing All Columns)\nOn the above DataFrame, we have a total of 10 rows with 2 rows having all values duplicated, performing distinct on this DataFrame should get us 9 after removing 1 duplicate row.\ndistinct() function on DataFrame returns a new DataFrame after removing the duplicate records. This example yields the below output.\n'''\n#Distinct\ndistinctDF = df.distinct()\nprint(\"Distinct count: \"+str(distinctDF.count()))\ndistinctDF.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65f36dd4-0496-41d8-a6bb-0c5479d4393e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Distinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Distinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\nAlternatively, you can also run dropDuplicates() function which returns a new DataFrame after removing duplicate rows.\n'''\n#Drop duplicates\ndf2 = df.dropDuplicates()\nprint(\"Distinct count: \"+str(df2.count()))\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0399af2d-1483-46d2-b641-2e9ec5fdc110"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Distinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Distinct count: 9\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2. PySpark Distinct of Selected Multiple Columns\nPySpark doesn’t have a distinct method which takes columns that should run distinct on (drop duplicate rows on selected multiple columns) however, it provides another signature of dropDuplicates() function which takes multiple columns to eliminate duplicates.\n\nNote that calling dropDuplicates() on DataFrame returns a new DataFrame with duplicate rows removed.\n\nYields below output. If you notice the output, It dropped 2 records that are duplicate.\n'''\n#Drop duplicates on selected columns\ndropDisDF = df.dropDuplicates([\"department\",\"salary\"])\nprint(\"Distinct count of department salary : \"+str(dropDisDF.count()))\ndropDisDF.show(truncate=False)\n\n'''\nIn this PySpark SQL article, you have learned distinct() method which is used to get the distinct values of rows (all columns) and also learned how to use dropDuplicates() to get the distinct and finally learned using dropDuplicates() function to get distinct of multiple columns.\n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d94321d3-0abf-4540-90ac-866960f61772"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Distinct count of department salary : 8\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Kumar        |Marketing |2000  |\n|Jeff         |Marketing |3000  |\n|James        |Sales     |3000  |\n|Robert       |Sales     |4100  |\n|Michael      |Sales     |4600  |\n+-------------+----------+------+\n\nOut[18]: '\\nIn this PySpark SQL article, you have learned distinct() method which is used to get the distinct values of rows (all columns) and also learned how to use dropDuplicates() to get the distinct and finally learned using dropDuplicates() function to get distinct of multiple columns.\\n'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Distinct count of department salary : 8\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|Maria        |Finance   |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Kumar        |Marketing |2000  |\n|Jeff         |Marketing |3000  |\n|James        |Sales     |3000  |\n|Robert       |Sales     |4100  |\n|Michael      |Sales     |4600  |\n+-------------+----------+------+\n\nOut[18]: '\\nIn this PySpark SQL article, you have learned distinct() method which is used to get the distinct values of rows (all columns) and also learned how to use dropDuplicates() to get the distinct and finally learned using dropDuplicates() function to get distinct of multiple columns.\\n'"]}}],"execution_count":0},{"cell_type":"code","source":["'''\nPySpark JSON Functions with Examples\nPySpark JSON functions are used to query or extract the elements from JSON string of DataFrame column by path, convert it to struct, mapt type e.t.c, In this article, I will explain the most used JSON SQL functions with Python examples.\n\n1. PySpark JSON Functions\nfrom_json() – Converts JSON string into Struct type or Map type.\n\nto_json() – Converts MapType or Struct type to JSON string.\n\njson_tuple() – Extract the Data from JSON and create them as a new columns.\n\nget_json_object() – Extracts JSON element from a JSON string based on json path specified.\n\nschema_of_json() – Create schema string from JSON string\n\n1.1. Create DataFrame with Column contains JSON String\nIn order to explain these JSON functions first, let’s create DataFrame with a column contains JSON string.\n'''\n\nfrom pyspark.sql import SparkSession,Row\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\njsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\ndf=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48221905-c85d-4f97-a782-951874c091ae"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+--------------------------------------------------------------------------+\n|id |value                                                                     |\n+---+--------------------------------------------------------------------------+\n|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n+---+--------------------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+--------------------------------------------------------------------------+\n|id |value                                                                     |\n+---+--------------------------------------------------------------------------+\n|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n+---+--------------------------------------------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.1. from_json()\nPySpark from_json() function is used to convert JSON string into Struct type or Map type. The below example converts JSON string to Map key-value pair. I will leave it to you to convert to struct type. Refer, Convert JSON string to Struct type column.\n'''\n\n#Convert JSON string column to Map type\nfrom pyspark.sql.types import MapType,StringType\nfrom pyspark.sql.functions import from_json\ndf2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\ndf2.printSchema()\ndf2.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b15b78c0-fe24-4e7c-b940-5ed29a42baf7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- id: long (nullable = true)\n |-- value: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+---+---------------------------------------------------------------------------+\n|id |value                                                                      |\n+---+---------------------------------------------------------------------------+\n|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n+---+---------------------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- id: long (nullable = true)\n |-- value: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+---+---------------------------------------------------------------------------+\n|id |value                                                                      |\n+---+---------------------------------------------------------------------------+\n|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n+---+---------------------------------------------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.2. to_json()\nto_json() function is used to convert DataFrame columns MapType or Struct type to JSON string. Here, I am using df2 that created from above from_json() example.\n'''\n\nfrom pyspark.sql.functions import to_json,col\ndf2.withColumn(\"value\",to_json(col(\"value\"))) \\\n   .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6305c622-ac16-4ec2-b39b-7cbbae3eb0ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+----------------------------------------------------------------------------+\n|id |value                                                                       |\n+---+----------------------------------------------------------------------------+\n|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n+---+----------------------------------------------------------------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+----------------------------------------------------------------------------+\n|id |value                                                                       |\n+---+----------------------------------------------------------------------------+\n|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n+---+----------------------------------------------------------------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.3. json_tuple()\nFunction json_tuple() is used the query or extract the elements from JSON column and create the result as a new columns.\n'''\n\nfrom pyspark.sql.functions import json_tuple\ndf.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n    .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9ce0294f-cda4-4488-8a54-9d88059ba049"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-------+-----------+-----------+\n|id |Zipcode|ZipCodeType|City       |\n+---+-------+-----------+-----------+\n|1  |704    |STANDARD   |PARC PARQUE|\n+---+-------+-----------+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-------+-----------+-----------+\n|id |Zipcode|ZipCodeType|City       |\n+---+-------+-----------+-----------+\n|1  |704    |STANDARD   |PARC PARQUE|\n+---+-------+-----------+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.4. get_json_object()\nget_json_object() is used to extract the JSON string based on path from the JSON column.\n'''\n\nfrom pyspark.sql.functions import get_json_object\ndf.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n    .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5554574f-0e60-4564-a60e-9ab8c602c9c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+-----------+\n|id |ZipCodeType|\n+---+-----------+\n|1  |STANDARD   |\n+---+-----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+-----------+\n|id |ZipCodeType|\n+---+-----------+\n|1  |STANDARD   |\n+---+-----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.5. schema_of_json()\nUse schema_of_json() to create schema string from JSON string column.\n'''\n\nfrom pyspark.sql.functions import schema_of_json,lit\nschemaStr=spark.range(1) \\\n    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n    .collect()[0][0]\nprint(schemaStr)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d31358e6-0206-449d-bb1b-7900fff1e221"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"STRUCT<City: STRING, State: STRING, ZipCodeType: STRING, Zipcode: BIGINT>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["STRUCT<City: STRING, State: STRING, ZipCodeType: STRING, Zipcode: BIGINT>\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\nPySpark Window Functions\nPySpark Window functions are used to calculate results such as the rank, row number e.t.c over a range of input rows. In this article, I’ve explained the concept of window functions, syntax, and finally how to use them with PySpark SQL and PySpark DataFrame API. These come in handy when we need to make aggregate operations in a specific window frame on DataFrame columns.\n\nWhen possible try to leverage standard library as they are little bit more compile-time safety, handles null and perform better when compared to UDF’s. If your application is critical on performance try to avoid using custom UDF at all costs as these are not guarantee on performance.\n\n1. Window Functions\nPySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row. PySpark SQL supports three kinds of window functions:\n\nranking functions\nanalytic functions\naggregate functions\n\nThe below table defines Ranking and Analytic functions and for aggregate functions, we can use any existing aggregate functions as a window function.\n\no perform an operation on a group first, we need to partition the data using Window.partitionBy() , and for row number and rank function we need to additionally order by on partition data using orderBy clause.\n\nClick on each link to know more about these functions along with the Scala examples.\n\n\n'''\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\nsimpleData = ((\"James\", \"Sales\", 3000), \\\n    (\"Michael\", \"Sales\", 4600),  \\\n    (\"Robert\", \"Sales\", 4100),   \\\n    (\"Maria\", \"Finance\", 3000),  \\\n    (\"James\", \"Sales\", 3000),    \\\n    (\"Scott\", \"Finance\", 3300),  \\\n    (\"Jen\", \"Finance\", 3900),    \\\n    (\"Jeff\", \"Marketing\", 3000), \\\n    (\"Kumar\", \"Marketing\", 2000),\\\n    (\"Saif\", \"Sales\", 4100) \\\n  )\n \ncolumns= [\"employee_name\", \"department\", \"salary\"]\n\ndf = spark.createDataFrame(data = simpleData, schema = columns)\n\ndf.printSchema()\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f0dee8a-b0da-484f-a861-1bd6f6809d3e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+-------------+----------+------+\n|employee_name|department|salary|\n+-------------+----------+------+\n|James        |Sales     |3000  |\n|Michael      |Sales     |4600  |\n|Robert       |Sales     |4100  |\n|Maria        |Finance   |3000  |\n|James        |Sales     |3000  |\n|Scott        |Finance   |3300  |\n|Jen          |Finance   |3900  |\n|Jeff         |Marketing |3000  |\n|Kumar        |Marketing |2000  |\n|Saif         |Sales     |4100  |\n+-------------+----------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.1 row_number Window Function\nrow_number() window function is used to give the sequential row number starting from 1 to the result of each window partition.\n'''\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nwindowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n\ndf.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n    .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd55ae21-f13b-4124-8888-30d77f15db48"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|Maria        |Finance   |3000  |1         |\n|Scott        |Finance   |3300  |2         |\n|Jen          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jeff         |Marketing |3000  |2         |\n|James        |Sales     |3000  |1         |\n|James        |Sales     |3000  |2         |\n|Robert       |Sales     |4100  |3         |\n|Saif         |Sales     |4100  |4         |\n|Michael      |Sales     |4600  |5         |\n+-------------+----------+------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----------+\n|employee_name|department|salary|row_number|\n+-------------+----------+------+----------+\n|Maria        |Finance   |3000  |1         |\n|Scott        |Finance   |3300  |2         |\n|Jen          |Finance   |3900  |3         |\n|Kumar        |Marketing |2000  |1         |\n|Jeff         |Marketing |3000  |2         |\n|James        |Sales     |3000  |1         |\n|James        |Sales     |3000  |2         |\n|Robert       |Sales     |4100  |3         |\n|Saif         |Sales     |4100  |4         |\n|Michael      |Sales     |4600  |5         |\n+-------------+----------+------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.2 rank Window Function\nrank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties.\n'''\nfrom pyspark.sql.functions import rank\ndf.withColumn(\"rank\",rank().over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"daa49b87-0d8a-41da-9ead-d85326f5861a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   1|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   3|\n|      Michael|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----+\n|employee_name|department|salary|rank|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|   1|\n|        Scott|   Finance|  3300|   2|\n|          Jen|   Finance|  3900|   3|\n|        Kumar| Marketing|  2000|   1|\n|         Jeff| Marketing|  3000|   2|\n|        James|     Sales|  3000|   1|\n|        James|     Sales|  3000|   1|\n|       Robert|     Sales|  4100|   3|\n|         Saif|     Sales|  4100|   3|\n|      Michael|     Sales|  4600|   5|\n+-------------+----------+------+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.3 dense_rank Window Function\ndense_rank() window function is used to get the result with rank of rows within a window partition without any gaps. This is similar to rank() function difference being rank function leaves gaps in rank when there are ties.\n'''\nfrom pyspark.sql.functions import dense_rank\ndf.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c85dcf33-a84e-4a55-99c7-cb6d503e0722"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----------+\n|employee_name|department|salary|dense_rank|\n+-------------+----------+------+----------+\n|        Maria|   Finance|  3000|         1|\n|        Scott|   Finance|  3300|         2|\n|          Jen|   Finance|  3900|         3|\n|        Kumar| Marketing|  2000|         1|\n|         Jeff| Marketing|  3000|         2|\n|        James|     Sales|  3000|         1|\n|        James|     Sales|  3000|         1|\n|       Robert|     Sales|  4100|         2|\n|         Saif|     Sales|  4100|         2|\n|      Michael|     Sales|  4600|         3|\n+-------------+----------+------+----------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.4 percent_rank Window Function\nThis is the same as the PERCENT_RANK function in SQL.\n'''\nfrom pyspark.sql.functions import percent_rank\ndf.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ada7790-a864-43a9-b5c9-16e25adffb29"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+------------+\n|employee_name|department|salary|percent_rank|\n+-------------+----------+------+------------+\n|        Maria|   Finance|  3000|         0.0|\n|        Scott|   Finance|  3300|         0.5|\n|          Jen|   Finance|  3900|         1.0|\n|        Kumar| Marketing|  2000|         0.0|\n|         Jeff| Marketing|  3000|         1.0|\n|        James|     Sales|  3000|         0.0|\n|        James|     Sales|  3000|         0.0|\n|       Robert|     Sales|  4100|         0.5|\n|         Saif|     Sales|  4100|         0.5|\n|      Michael|     Sales|  4600|         1.0|\n+-------------+----------+------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+------------+\n|employee_name|department|salary|percent_rank|\n+-------------+----------+------+------------+\n|        Maria|   Finance|  3000|         0.0|\n|        Scott|   Finance|  3300|         0.5|\n|          Jen|   Finance|  3900|         1.0|\n|        Kumar| Marketing|  2000|         0.0|\n|         Jeff| Marketing|  3000|         1.0|\n|        James|     Sales|  3000|         0.0|\n|        James|     Sales|  3000|         0.0|\n|       Robert|     Sales|  4100|         0.5|\n|         Saif|     Sales|  4100|         0.5|\n|      Michael|     Sales|  4600|         1.0|\n+-------------+----------+------+------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n2.5 ntile Window Function\nntile() window function returns the relative rank of result rows within a window partition. In below example we have used 2 as an argument to ntile hence it returns ranking between 2 values (1 and 2)\n'''\nfrom pyspark.sql.functions import ntile\ndf.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94c0fae5-2b9b-47e7-82a8-76fc1fadde0c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+-----+\n|employee_name|department|salary|ntile|\n+-------------+----------+------+-----+\n|        Maria|   Finance|  3000|    1|\n|        Scott|   Finance|  3300|    1|\n|          Jen|   Finance|  3900|    2|\n|        Kumar| Marketing|  2000|    1|\n|         Jeff| Marketing|  3000|    2|\n|        James|     Sales|  3000|    1|\n|        James|     Sales|  3000|    1|\n|       Robert|     Sales|  4100|    1|\n|         Saif|     Sales|  4100|    2|\n|      Michael|     Sales|  4600|    2|\n+-------------+----------+------+-----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+-----+\n|employee_name|department|salary|ntile|\n+-------------+----------+------+-----+\n|        Maria|   Finance|  3000|    1|\n|        Scott|   Finance|  3300|    1|\n|          Jen|   Finance|  3900|    2|\n|        Kumar| Marketing|  2000|    1|\n|         Jeff| Marketing|  3000|    2|\n|        James|     Sales|  3000|    1|\n|        James|     Sales|  3000|    1|\n|       Robert|     Sales|  4100|    1|\n|         Saif|     Sales|  4100|    2|\n|      Michael|     Sales|  4600|    2|\n+-------------+----------+------+-----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n3.1 cume_dist Window Function\ncume_dist() window function is used to get the cumulative distribution of values within a window partition.\n\nThis is the same as the DENSE_RANK function in SQL.\n'''\nfrom pyspark.sql.functions import cume_dist    \ndf.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n   .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71fc9904-55a4-45e1-ac0d-1e5c6d49afca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+------------------+\n|employee_name|department|salary|         cume_dist|\n+-------------+----------+------+------------------+\n|        Maria|   Finance|  3000|0.3333333333333333|\n|        Scott|   Finance|  3300|0.6666666666666666|\n|          Jen|   Finance|  3900|               1.0|\n|        Kumar| Marketing|  2000|               0.5|\n|         Jeff| Marketing|  3000|               1.0|\n|        James|     Sales|  3000|               0.4|\n|        James|     Sales|  3000|               0.4|\n|       Robert|     Sales|  4100|               0.8|\n|         Saif|     Sales|  4100|               0.8|\n|      Michael|     Sales|  4600|               1.0|\n+-------------+----------+------+------------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+------------------+\n|employee_name|department|salary|         cume_dist|\n+-------------+----------+------+------------------+\n|        Maria|   Finance|  3000|0.3333333333333333|\n|        Scott|   Finance|  3300|0.6666666666666666|\n|          Jen|   Finance|  3900|               1.0|\n|        Kumar| Marketing|  2000|               0.5|\n|         Jeff| Marketing|  3000|               1.0|\n|        James|     Sales|  3000|               0.4|\n|        James|     Sales|  3000|               0.4|\n|       Robert|     Sales|  4100|               0.8|\n|         Saif|     Sales|  4100|               0.8|\n|      Michael|     Sales|  4600|               1.0|\n+-------------+----------+------+------------------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n3.2 lag Window Function\nThis is the same as the LAG function in SQL.\n'''\nfrom pyspark.sql.functions import lag    \ndf.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n      .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"390cb20c-b2c6-4f6a-9852-df5e2a020db0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|null|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|3000|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|null|\n|        James|     Sales|  3000|null|\n|       Robert|     Sales|  4100|3000|\n|         Saif|     Sales|  4100|3000|\n|      Michael|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----+\n|employee_name|department|salary| lag|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|null|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|3000|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|null|\n|        James|     Sales|  3000|null|\n|       Robert|     Sales|  4100|3000|\n|         Saif|     Sales|  4100|3000|\n|      Michael|     Sales|  4600|4100|\n+-------------+----------+------+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\n3.3 lead Window Function\nThis is the same as the LEAD function in SQL.\n'''\nfrom pyspark.sql.functions import lead    \ndf.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n    .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d2e6450-6eb7-4625-87c1-40e2470d42c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|3900|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|4100|\n|        James|     Sales|  3000|4100|\n|       Robert|     Sales|  4100|4600|\n|         Saif|     Sales|  4100|null|\n|      Michael|     Sales|  4600|null|\n+-------------+----------+------+----+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------------+----------+------+----+\n|employee_name|department|salary|lead|\n+-------------+----------+------+----+\n|        Maria|   Finance|  3000|3900|\n|        Scott|   Finance|  3300|null|\n|          Jen|   Finance|  3900|null|\n|        Kumar| Marketing|  2000|null|\n|         Jeff| Marketing|  3000|null|\n|        James|     Sales|  3000|4100|\n|        James|     Sales|  3000|4100|\n|       Robert|     Sales|  4100|4600|\n|         Saif|     Sales|  4100|null|\n|      Michael|     Sales|  4600|null|\n+-------------+----------+------+----+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["'''\nPySpark Window Aggregate Functions\nIn this section, I will explain how to calculate sum, min, max for each department using PySpark SQL Aggregate window functions and WindowSpec. When working with Aggregate functions, we don’t need to use order by clause.\n'''\nwindowSpecAgg  = Window.partitionBy(\"department\")\nfrom pyspark.sql.functions import col,avg,sum,min,max,row_number \ndf.withColumn(\"row\",row_number().over(windowSpec)) \\\n  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n  .show()\n'''\nIn this section, you have learned what are PySpark SQL Window functions their syntax and how to use them with aggregate function along with several examples in python.\n\nReferences\n\nI would recommend reading Window Functions Introduction and SQL Window Functions API blogs for a further understanding of Windows functions. Also, refer to SQL Window functions to know window functions from native SQL.\n'''\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d8640ed-f491-4048-8a1c-71ed477b6681"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n|     Sales|3760.0|18800|3000|4600|\n+----------+------+-----+----+----+\n\nOut[34]: '\\nIn this section, you have learned what are PySpark SQL Window functions their syntax and how to use them with aggregate function along with several examples in python.\\n\\nReferences\\n\\nI would recommend reading Window Functions Introduction and SQL Window Functions API blogs for a further understanding of Windows functions. Also, refer to SQL Window functions to know window functions from native SQL.\\n'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+------+-----+----+----+\n|department|   avg|  sum| min| max|\n+----------+------+-----+----+----+\n|   Finance|3400.0|10200|3000|3900|\n| Marketing|2500.0| 5000|2000|3000|\n|     Sales|3760.0|18800|3000|4600|\n+----------+------+-----+----+----+\n\nOut[34]: '\\nIn this section, you have learned what are PySpark SQL Window functions their syntax and how to use them with aggregate function along with several examples in python.\\n\\nReferences\\n\\nI would recommend reading Window Functions Introduction and SQL Window Functions API blogs for a further understanding of Windows functions. Also, refer to SQL Window functions to know window functions from native SQL.\\n'"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"PySpark join","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1578864186662360}},"nbformat":4,"nbformat_minor":0}
